{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Time Series Forecasting Using Multilayer Perceptron (MLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Description\n",
    "This tutorial will showcase the usage of multilayer perceptron (MLP) in time series forecasting. The deep learning framework used here is PyTorch. An exercise section is attached for you to practice and hone your skills. Do make good use of it.\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Prepare dataset to be feed into MLP model\n",
    "2. Build and apply MLP model to perform forecasting\n",
    "3. Compare and contrast performance of MLP model\n",
    "\n",
    "## Notebook Outline\n",
    "Below is the outline for this tutorial:\n",
    "1. [Notebook Configurations](#configuration)\n",
    "2. [Dataset](#dataset)\n",
    "3. [Basic Analytics](#analytics)\n",
    "4. [Model Development](#model-dev)\n",
    "5. [Evaluation](#evaluation)\n",
    "6. [Exercise](#exercise)\n",
    "7. [Reference](#reference)\n",
    "8. [Bonus](#bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"configuration\">Notebook Configurations</a>\n",
    "Following are the modules that will be used for this tutorial. This notebook will heavily use `torch` as they provide great APIs when for building deep learning models. You can find out more about them [here](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f343c8eee90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# seed configuration\n",
    "np.random.seed(38)\n",
    "torch.manual_seed(38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"dataset\">Dataset</a>\n",
    "We would be using the a time series data containing monthly shampoo sales for this exercise. It is a famous and commonly used dataset for time series forecasting practice. We will split the data into training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nevermind/Desktop/datasets/others/shampoo-sales.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19601/2987001424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# import dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_shampoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtraining_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/nevermind/D/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nevermind/D/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nevermind/D/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nevermind/D/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nevermind/D/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nevermind/D/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nevermind/Desktop/datasets/others/shampoo-sales.csv'"
     ]
    }
   ],
   "source": [
    "# path specification\n",
    "dataset_dir_path = Path().resolve().parent.parent/'datasets'\n",
    "data_dir_path = dataset_dir_path/'others'\n",
    "training_file_name = 'shampoo-sales.csv' \n",
    "\n",
    "# import dataset\n",
    "df_shampoo = pd.read_csv(data_dir_path/training_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"analytics\">Basic Analytics</a>\n",
    "Let's do some basic analytics and see what nugget can we find out of the training dataset! Let first find out the number of rows and columns of the training dataset.\n",
    "\n",
    "**Note: Since we are not supposed to know any information about the test dataset, we will not perform any analytics on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect first 15 rows of data\n",
    "df_shampoo.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are two columns, month and sales. Let's also inspect the data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dtype\n",
    "for column in df_shampoo.columns:\n",
    "    print(f\"The data type for '{column}' column is: {df_shampoo[column].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute descriptive statistics\n",
    "df_shampoo.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive statistics computed are the mean, standard deviation, minimum and maximum values. We see that the maximum value is 682.00 and minimum value is 119.30, and the standard deviation is 148.94. This shows that the sales volume varies by a large spread. Next, we will plot a time plot for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time plot \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18, 7)\n",
    "ax.set_title(\"Shampoo Sales against Time\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.plot(\n",
    "    df_shampoo['sales'],\n",
    "    color='blue', label='Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we get to feel the ebb and flow of the data. Let's split the dataset into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "df_train, df_test = train_test_split(df_shampoo, train_size=0.7, shuffle=False)\n",
    "\n",
    "# display training dataset length\n",
    "print(f\"The length of training dataset is: {len(df_train)}\")\n",
    "\n",
    "# display test dataset length\n",
    "print(f\"The length of test dataset is: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the portion of dataset which is splitted as training dataset and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time plot for training and test dataset \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18, 7)\n",
    "ax.set_title(\"Shampoo Sales against Time\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.plot(\n",
    "    df_train['sales'],\n",
    "    color='blue', label='Training data');\n",
    "ax.plot(\n",
    "    df_test['sales'],\n",
    "    color='goldenrod', label='Test data')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scalinng also help in shortening model training time. Let's do that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(df_train['sales'].values.reshape(-1,1))\n",
    "test_scaled = scaler.transform(df_test['sales'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"model-dev\">Model Development</a>\n",
    "The `torch` library provides a number of API to easily construct a MLP model for the tasks at hand. But first, we will need to prepare the dataset into a format ingestible by the model first. We will first transform the time series data into feature of label, where the feature would be a sequence of *n* length of data and label would be lead-1 of the last data in the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a helper function which slices through the features using the sequence_length parameter and index the labe at the time step\n",
    "# function should return features in sequence and labels\n",
    "def sequencing_data(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Helper function to sample sub-sequence of training data.\n",
    "    Input data must be in np.ndarray.\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range(data.shape[0] - sequence_length):\n",
    "\n",
    "        # copy the sequences of data starting at this index\n",
    "        x.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return x, y\n",
    "\n",
    "# calling the helper function and store them in variables\n",
    "X_sequence_train, y_sequence_train = sequencing_data(train_scaled, 3)\n",
    "X_sequence_test, y_sequence_test = sequencing_data(test_scaled, 3)\n",
    "\n",
    "# sanity check \n",
    "print(\"Total samples for X train: \" + str(len(X_sequence_train)))\n",
    "print(\"Total samples for y train: \" + str(len(y_sequence_train)))\n",
    "print(\"Total samples for X test: \" + str(len(X_sequence_test)))\n",
    "print(\"Total samples for y test: \" + str(len(y_sequence_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a custom `Dataset` class to transfrom the data into tuples of `torch.Tensor`. This is necessary if we want to load the data in batches using `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShampooDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Convert input data into torch FloatTensor. \n",
    "    Inherit Dataset class. Return length of instance when len method is called and return specific sample\n",
    "    of data when indexed.\n",
    "    \"\"\" \n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write the codes to convert data into `Dataset` object and then into `DataLoader` object. We will be using a batch size of 4 for this `DataLoader` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare batch size\n",
    "batch_size = 4\n",
    "\n",
    "# convert into Dataset using customized class\n",
    "train_Dataset = ShampooDataset(X_sequence_train, y_sequence_train)\n",
    "test_Dataset = ShampooDataset(X_sequence_test, y_sequence_test)\n",
    "\n",
    "# convert into DataLoader\n",
    "train_loader = DataLoader(train_Dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_Dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# declare a dict object\n",
    "dataloaders = {'train': train_loader, 'test': test_loader} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's about time to declare our MLP model architecture. The following code demonstrates how to build a simple MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP model with configurable input size and output size.\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input = nn.Linear(input_size, 64)\n",
    "        self.linear1 = nn.Linear(64, 16)\n",
    "        self.output = nn.Linear(16, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 3)\n",
    "        out = F.relu(self.input(x))\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our training, we also need to configure some hyperparameters. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "torch.manual_seed(38)\n",
    "\n",
    "# setting hyperparameter\n",
    "input_size = 3\n",
    "output_size = 1\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# instantiate the model\n",
    "model = MLP(input_size, output_size)\n",
    "\n",
    "# continue setting hyperparameter\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform our model training now. In `torch`, we need to manually write codes to implement model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement model training and validation loop\n",
    "loss_score = {'train': [], 'test': []}\n",
    "for epoch in range(epochs):\n",
    "#     print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        loss_cumsum = 0.0\n",
    "        total_iterations = 0.0\n",
    "\n",
    "        for i, (X, y) in enumerate(dataloaders[phase]):\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                out = model(X)\n",
    "                loss = criterion(out.squeeze(), y.squeeze())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            loss_cumsum += loss.item() * out.size(0)\n",
    "            total_iterations += out.size(0)\n",
    "\n",
    "        epoch_loss = loss_cumsum / total_iterations\n",
    "#         print(f'{phase.upper()} Loss: {epoch_loss}')\n",
    "        loss_score[phase].append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the loss score to check how is the model performing when trained by epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(14, 7)\n",
    "ax.set_title(\"Loss Score against Epoch\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Epoch Number\")\n",
    "ax.set_ylabel(\"Loss Score\")\n",
    "\n",
    "ax.plot(loss_score['train'], color='goldenrod', label='Training Loss')\n",
    "ax.plot(loss_score['test'], color='green', label='Test Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"evaluation\">Evaluation</a>\n",
    "There are a few metrics that can be used to evaluate the performance of a regression model, which include:\n",
    "* mean squared error (MSE)\n",
    "* root mean squared error (RMSE)\n",
    "* mean absolute error (MAE)\n",
    "\n",
    "We will examine the performance of the model on test set using RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "count = 0\n",
    "\n",
    "# model predictions\n",
    "with torch.no_grad():\n",
    "    for i, (X, y) in enumerate(DataLoader(test_Dataset, 1)):\n",
    "        count += 1\n",
    "        output = model(X)\n",
    "        predictions.append(output.item())\n",
    "        \n",
    "# convert list into np array and inverse transform it into proper values before feature scaling\n",
    "predictions = np.array(predictions)\n",
    "predictions = scaler.inverse_transform(predictions.reshape(1,-1))\n",
    "\n",
    "# remove beginning 3 rows of data since at least 3 data are needed to make a prediction\n",
    "df_test = df_test.iloc[3:]\n",
    "df_test['preds'] = predictions.squeeze()\n",
    "\n",
    "# compute MSE\n",
    "RMSE = mean_squared_error(df_test.preds, df_test.sales, squared=False)\n",
    "print(f\"The RMSE for the model is {RMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print and visualize predictions made by the model for inspection purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test['preds'])\n",
    "\n",
    "# plot time plot for predictions and ground truth \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18, 7)\n",
    "ax.set_title(\"Shampoo Sales against Time\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.plot(\n",
    "    df_test['sales'],\n",
    "    color='blue', label='Actual Values');\n",
    "ax.plot(\n",
    "    df_test['preds'],\n",
    "    color='goldenrod', label='Predicted Values')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"exercise\">Exercise</a>\n",
    "\n",
    "This exercise section is attached for you to practice and hone your skills. Try your best effort to complete it without referring to the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Configurations\n",
    "You can import any module that you wish to use for this exercise. However, if you run the cells from the start of this notebook, most probably all the modules which you need had already been imported at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "**INSTRUCTIONS**: Please perform time series forecasting using MLP on furniture-sales.csv dataset. You can locate the file in the `datasets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path specification\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# import dataset\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Analytics\n",
    "**INSTRUCTIONS**: Perform some basic analytics on the dataset by following the guidelines that are provided in each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect first 15 rows of data\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dtype\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column to 'Date' and 'Sales'\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# compute descriptive statistics\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time plot \n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 7:3 of train:test set from the whole dataset\n",
    "### BEGIN SOLUTION\n",
    "df_train, df_test = None, None\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time plot for training and test dataset \n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling using min-max normalization\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "**INSTRUCTIONS**: Build and develop a MLP model using training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a helper function which slices through the features using the sequence_length parameter and index the labe at the time step\n",
    "# function should return features in sequence and labels\n",
    "def sequencing_data(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Helper function to sample sub-sequence of training data.\n",
    "    Input data must be in np.ndarray.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    x = None\n",
    "    y = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return x, y\n",
    "\n",
    "# calling the helper function and store them in variables\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write custom Dataset to transform list object into Dataset object\n",
    "class FurnitureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Convert input data into torch FloatTensor. \n",
    "    Inherit Dataset class. Return length of instance when len method is called and return specific sample\n",
    "    of data when indexed.\n",
    "    \"\"\" \n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare batch size\n",
    "### BEGIN SOLUTION\n",
    "batch_size = None\n",
    "### END SOLUTION\n",
    "\n",
    "# convert into Dataset using customized class\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# convert into DataLoader\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# declare a dict object\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP model architecture\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP model with configurable input size and output size.\"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameter\n",
    "### BEGIN SOLUTION\n",
    "input_size = None\n",
    "output_size = None\n",
    "epochs = None\n",
    "learning_rate = None\n",
    "### END SOLUTION\n",
    "\n",
    "# instantiate the model\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# continue setting hyperparameter\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement model training and validation loop\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss scores\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "**INSTRUCTIONS:** Follow the guidelines given in the cells to evaluate performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model predictions\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "        \n",
    "# convert list into np array and inverse transform it into proper values before feature scaling\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# remove beginning 7 rows of data since at least 7 data are needed to make a prediction\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# compute MSE\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print prediction results\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# plot time plot for predictions and ground truth \n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"reference\">Reference</a>\n",
    "1. [Pytorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"bonus\">Bonus</a>\n",
    "\n",
    "The first neural network to be applied to a real-world problem is MADALINE, which stands for Multiple ADAptive LINear Elements. It was developed by Bernard Widrow and Marcian Hoff of Stanfor from 1959.\n",
    "\n",
    "Source: [3.0 History of Neural Networks](http://www2.psych.utoronto.ca/users/reingold/courses/ai/cache/neural4.html#:~:text=In%201959%2C%20Bernard%20Widrow%20and,to%20a%20real%20world%20problem.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
